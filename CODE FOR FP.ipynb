{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyME0Tcd3ByRV8ehZ2fU917u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nisreen28/3breast_cancer_SVC_ECE.ipynb/blob/main/CODE%20FOR%20FP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2rFkyGnX2GP",
        "outputId": "70a288e6-3598-4f4c-d1ca-7b8b5cfa686a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "KNetSeg Epoch 1/50 | Train Loss: 0.7213 (BCE: 0.1729, Dice: 0.8584) | Val Loss: 0.4750 (BCE: 0.0248, Dice: 0.5875) | Val Dice Score: 0.4125\n",
            "KNetSeg Epoch 2/50 | Train Loss: 0.2229 (BCE: 0.0109, Dice: 0.2759) | Val Loss: 0.1280 (BCE: 0.0082, Dice: 0.1579) | Val Dice Score: 0.8421\n",
            "KNetSeg Epoch 3/50 | Train Loss: 0.1040 (BCE: 0.0077, Dice: 0.1281) | Val Loss: 0.1112 (BCE: 0.0093, Dice: 0.1367) | Val Dice Score: 0.8633\n",
            "KNetSeg Epoch 4/50 | Train Loss: 0.0835 (BCE: 0.0070, Dice: 0.1027) | Val Loss: 0.0775 (BCE: 0.0073, Dice: 0.0950) | Val Dice Score: 0.9050\n",
            "KNetSeg Epoch 5/50 | Train Loss: 0.0690 (BCE: 0.0066, Dice: 0.0845) | Val Loss: 0.0825 (BCE: 0.0099, Dice: 0.1006) | Val Dice Score: 0.8994\n",
            "KNetSeg Epoch 6/50 | Train Loss: 0.0625 (BCE: 0.0061, Dice: 0.0767) | Val Loss: 0.0638 (BCE: 0.0068, Dice: 0.0780) | Val Dice Score: 0.9220\n",
            "KNetSeg Epoch 7/50 | Train Loss: 0.0557 (BCE: 0.0062, Dice: 0.0681) | Val Loss: 0.0540 (BCE: 0.0059, Dice: 0.0660) | Val Dice Score: 0.9340\n",
            "KNetSeg Epoch 8/50 | Train Loss: 0.0524 (BCE: 0.0059, Dice: 0.0640) | Val Loss: 0.0856 (BCE: 0.0105, Dice: 0.1044) | Val Dice Score: 0.8957\n",
            "KNetSeg Epoch 9/50 | Train Loss: 0.0521 (BCE: 0.0060, Dice: 0.0636) | Val Loss: 0.0492 (BCE: 0.0056, Dice: 0.0601) | Val Dice Score: 0.9399\n",
            "KNetSeg Epoch 10/50 | Train Loss: 0.0467 (BCE: 0.0055, Dice: 0.0570) | Val Loss: 0.0553 (BCE: 0.0070, Dice: 0.0674) | Val Dice Score: 0.9326\n",
            "KNetSeg Epoch 11/50 | Train Loss: 0.0455 (BCE: 0.0055, Dice: 0.0555) | Val Loss: 0.0523 (BCE: 0.0065, Dice: 0.0637) | Val Dice Score: 0.9363\n",
            "KNetSeg Epoch 12/50 | Train Loss: 0.0468 (BCE: 0.0059, Dice: 0.0570) | Val Loss: 0.0546 (BCE: 0.0070, Dice: 0.0665) | Val Dice Score: 0.9335\n",
            "KNetSeg Early stopping triggered\n",
            "KNetSeg Test Loss: 0.0444 (BCE: 0.0056, Dice: 0.0610) | Test Dice Score: 0.9409 | Precision: 0.9786 | Recall: 0.9075 | F1: 0.9417 | IoU: 0.8899 | Specificity: 0.9999\n",
            "KNetSeg Confusion Matrix: TN=13016239, FP=1766, FN=8250, TP=80945\n",
            "BaselineUNet Epoch 1/50 | Train Loss: 0.5111 (BCE: 0.0565, Dice: 0.6247) | Val Loss: 0.1720 (BCE: 0.0158, Dice: 0.2110) | Val Dice Score: 0.7890\n",
            "BaselineUNet Epoch 2/50 | Train Loss: 0.1598 (BCE: 0.0169, Dice: 0.1956) | Val Loss: 0.1559 (BCE: 0.0173, Dice: 0.1905) | Val Dice Score: 0.8095\n",
            "BaselineUNet Epoch 3/50 | Train Loss: 0.1486 (BCE: 0.0182, Dice: 0.1812) | Val Loss: 0.1507 (BCE: 0.0204, Dice: 0.1833) | Val Dice Score: 0.8167\n",
            "BaselineUNet Epoch 4/50 | Train Loss: 0.1362 (BCE: 0.0178, Dice: 0.1658) | Val Loss: 0.1307 (BCE: 0.0166, Dice: 0.1592) | Val Dice Score: 0.8408\n",
            "BaselineUNet Epoch 5/50 | Train Loss: 0.1296 (BCE: 0.0174, Dice: 0.1577) | Val Loss: 0.1309 (BCE: 0.0175, Dice: 0.1592) | Val Dice Score: 0.8408\n",
            "BaselineUNet Epoch 6/50 | Train Loss: 0.1234 (BCE: 0.0155, Dice: 0.1503) | Val Loss: 0.1244 (BCE: 0.0145, Dice: 0.1518) | Val Dice Score: 0.8482\n",
            "BaselineUNet Epoch 7/50 | Train Loss: 0.1233 (BCE: 0.0152, Dice: 0.1503) | Val Loss: 0.1180 (BCE: 0.0130, Dice: 0.1443) | Val Dice Score: 0.8557\n",
            "BaselineUNet Epoch 8/50 | Train Loss: 0.1177 (BCE: 0.0133, Dice: 0.1438) | Val Loss: 0.1086 (BCE: 0.0125, Dice: 0.1327) | Val Dice Score: 0.8673\n",
            "BaselineUNet Epoch 9/50 | Train Loss: 0.1077 (BCE: 0.0124, Dice: 0.1315) | Val Loss: 0.1002 (BCE: 0.0109, Dice: 0.1225) | Val Dice Score: 0.8776\n",
            "BaselineUNet Epoch 10/50 | Train Loss: 0.0980 (BCE: 0.0121, Dice: 0.1195) | Val Loss: 0.0973 (BCE: 0.0121, Dice: 0.1186) | Val Dice Score: 0.8813\n",
            "BaselineUNet Epoch 11/50 | Train Loss: 0.0908 (BCE: 0.0119, Dice: 0.1105) | Val Loss: 0.0849 (BCE: 0.0104, Dice: 0.1035) | Val Dice Score: 0.8966\n",
            "BaselineUNet Epoch 12/50 | Train Loss: 0.0815 (BCE: 0.0109, Dice: 0.0992) | Val Loss: 0.0779 (BCE: 0.0102, Dice: 0.0948) | Val Dice Score: 0.9052\n",
            "BaselineUNet Epoch 13/50 | Train Loss: 0.0784 (BCE: 0.0100, Dice: 0.0955) | Val Loss: 0.0730 (BCE: 0.0084, Dice: 0.0892) | Val Dice Score: 0.9108\n",
            "BaselineUNet Epoch 14/50 | Train Loss: 0.0771 (BCE: 0.0102, Dice: 0.0939) | Val Loss: 0.0867 (BCE: 0.0141, Dice: 0.1048) | Val Dice Score: 0.8952\n",
            "BaselineUNet Epoch 15/50 | Train Loss: 0.0715 (BCE: 0.0098, Dice: 0.0869) | Val Loss: 0.0758 (BCE: 0.0087, Dice: 0.0925) | Val Dice Score: 0.9075\n",
            "BaselineUNet Epoch 16/50 | Train Loss: 0.0654 (BCE: 0.0089, Dice: 0.0795) | Val Loss: 0.0627 (BCE: 0.0082, Dice: 0.0763) | Val Dice Score: 0.9237\n",
            "BaselineUNet Epoch 17/50 | Train Loss: 0.0640 (BCE: 0.0091, Dice: 0.0777) | Val Loss: 0.0615 (BCE: 0.0075, Dice: 0.0750) | Val Dice Score: 0.9250\n",
            "BaselineUNet Epoch 18/50 | Train Loss: 0.0620 (BCE: 0.0086, Dice: 0.0753) | Val Loss: 0.0570 (BCE: 0.0075, Dice: 0.0693) | Val Dice Score: 0.9307\n",
            "BaselineUNet Epoch 19/50 | Train Loss: 0.0602 (BCE: 0.0086, Dice: 0.0731) | Val Loss: 0.0572 (BCE: 0.0077, Dice: 0.0695) | Val Dice Score: 0.9305\n",
            "BaselineUNet Epoch 20/50 | Train Loss: 0.0581 (BCE: 0.0079, Dice: 0.0706) | Val Loss: 0.0581 (BCE: 0.0067, Dice: 0.0709) | Val Dice Score: 0.9291\n",
            "BaselineUNet Epoch 21/50 | Train Loss: 0.0563 (BCE: 0.0076, Dice: 0.0685) | Val Loss: 0.0817 (BCE: 0.0138, Dice: 0.0987) | Val Dice Score: 0.9014\n",
            "BaselineUNet Epoch 22/50 | Train Loss: 0.0552 (BCE: 0.0081, Dice: 0.0669) | Val Loss: 0.0550 (BCE: 0.0077, Dice: 0.0668) | Val Dice Score: 0.9332\n",
            "BaselineUNet Epoch 23/50 | Train Loss: 0.0537 (BCE: 0.0073, Dice: 0.0653) | Val Loss: 0.0514 (BCE: 0.0063, Dice: 0.0627) | Val Dice Score: 0.9373\n",
            "BaselineUNet Epoch 24/50 | Train Loss: 0.0569 (BCE: 0.0079, Dice: 0.0692) | Val Loss: 0.0509 (BCE: 0.0081, Dice: 0.0616) | Val Dice Score: 0.9384\n",
            "BaselineUNet Epoch 25/50 | Train Loss: 0.6247 (BCE: 0.7935, Dice: 0.5825) | Val Loss: 0.8262 (BCE: 0.1310, Dice: 1.0000) | Val Dice Score: 0.0000\n",
            "BaselineUNet Epoch 26/50 | Train Loss: 0.7230 (BCE: 0.0443, Dice: 0.8926) | Val Loss: 0.1874 (BCE: 0.0171, Dice: 0.2299) | Val Dice Score: 0.7701\n",
            "BaselineUNet Epoch 27/50 | Train Loss: 0.1571 (BCE: 0.0161, Dice: 0.1924) | Val Loss: 0.1453 (BCE: 0.0156, Dice: 0.1777) | Val Dice Score: 0.8223\n",
            "BaselineUNet Epoch 28/50 | Train Loss: 0.1429 (BCE: 0.0157, Dice: 0.1747) | Val Loss: 0.1521 (BCE: 0.0150, Dice: 0.1864) | Val Dice Score: 0.8136\n",
            "BaselineUNet Early stopping triggered\n",
            "BaselineUNet Test Loss: 0.1291 (BCE: 0.0150, Dice: 0.1780) | Test Dice Score: 0.8266 | Precision: 0.8610 | Recall: 0.7968 | F1: 0.8277 | IoU: 0.7060 | Specificity: 0.9991\n",
            "BaselineUNet Confusion Matrix: TN=13004577, FP=11694, FN=18477, TP=72452\n",
            "Training and testing complete for both models. Results and comparisons saved in /content/drive/MyDrive/LeishManiaPlots\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchsummary import summary\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
        "import seaborn as sns\n",
        "from torch.amp import GradScaler, autocast\n",
        "from numba import jit\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Device and Output Directory\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/LeishManiaPlots\"  # Save to Google Drive\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Part 1: Data Preparation, Preprocessing and Visualization\n",
        "def generate_synthetic_data(img_size=256, num_objects=20, uninfected=False):\n",
        "    image = np.random.normal(0.5, 0.1, (img_size, img_size)).astype(np.float32)\n",
        "    mask = np.zeros((img_size, img_size), dtype=np.float32)\n",
        "    is_infected = not uninfected\n",
        "    if is_infected:\n",
        "        for _ in range(num_objects + np.random.randint(-5, 5)):\n",
        "            center = (np.random.randint(20, img_size-20), np.random.randint(20, img_size-20))\n",
        "            radius = np.random.randint(2, 5)\n",
        "            cv2.circle(image, center, radius, 0.8, -1)\n",
        "            cv2.circle(mask, center, radius, 1.0, -1)\n",
        "    if np.random.rand() > 0.5:\n",
        "        flip_code = np.random.randint(0, 2)\n",
        "        image = cv2.flip(image, flip_code)\n",
        "        mask = cv2.flip(mask, flip_code)\n",
        "    angle = np.random.choice([0, 90, 180, 270])\n",
        "    M = cv2.getRotationMatrix2D((img_size//2, img_size//2), angle, 1)\n",
        "    image = cv2.warpAffine(image, M, (img_size, img_size))\n",
        "    mask = cv2.warpAffine(mask, M, (img_size, img_size))\n",
        "    return image, mask, is_infected\n",
        "\n",
        "def add_gaussian_noise(image, std=0.2):\n",
        "    noise = np.random.normal(0, std, image.shape).astype(np.float32)\n",
        "    return np.clip(image + noise, 0, 1)\n",
        "\n",
        "@jit(nopython=True)\n",
        "def kalman_filter(image, a=0.3, b=0.3):\n",
        "    h, w = image.shape\n",
        "    filtered = np.zeros((h, w), dtype=np.float32)\n",
        "    for m in range(h):\n",
        "        for n in range(w):\n",
        "            left = filtered[m-1, n] if m > 0 else image[m, n]\n",
        "            top = filtered[m, n-1] if n > 0 else image[m, n]\n",
        "            filtered[m, n] = a * left + b * top + (1 - a - b) * image[m, n]\n",
        "    return filtered\n",
        "\n",
        "def sobel_edges(image):\n",
        "    sobel_x = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)\n",
        "    sobel_y = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)\n",
        "    magnitude = np.sqrt(sobel_x**2 + sobel_y**2)\n",
        "    return (magnitude / magnitude.max()).astype(np.float32)\n",
        "\n",
        "def visualize_preprocessing(clean, noisy, denoised, edges, mask, is_infected, prefix=\"example\"):\n",
        "    fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
        "    axes[0].imshow(clean, cmap='gray'); axes[0].set_title('Clean Image')\n",
        "    axes[1].imshow(noisy, cmap='gray'); axes[1].set_title('Noisy Image')\n",
        "    axes[2].imshow(denoised, cmap='gray'); axes[2].set_title('Denoised (Kalman)')\n",
        "    axes[3].imshow(edges, cmap='gray'); axes[3].set_title('Sobel Edges')\n",
        "    axes[4].imshow(mask, cmap='gray'); axes[4].set_title('Ground Truth Mask')\n",
        "    for ax in axes: ax.axis('off')\n",
        "    status = \"Infected\" if is_infected else \"Uninfected\"\n",
        "    plt.suptitle(f\"{prefix} - {status}\", fontsize=16)\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, f\"{prefix}_preprocessing.png\"))\n",
        "    plt.close()\n",
        "\n",
        "clean_inf, mask_inf, is_infected_inf = generate_synthetic_data(uninfected=False)\n",
        "noisy_inf = add_gaussian_noise(clean_inf, std=0.2)\n",
        "denoised_inf = kalman_filter(noisy_inf, a=0.3, b=0.3)\n",
        "edges_inf = sobel_edges(denoised_inf)\n",
        "visualize_preprocessing(clean_inf, noisy_inf, denoised_inf, edges_inf, mask_inf, is_infected_inf, prefix=\"infected\")\n",
        "\n",
        "clean_uninf, mask_uninf, is_infected_uninf = generate_synthetic_data(uninfected=True)\n",
        "noisy_uninf = add_gaussian_noise(clean_uninf, std=0.2)\n",
        "denoised_uninf = kalman_filter(noisy_uninf, a=0.3, b=0.3)\n",
        "edges_uninf = sobel_edges(denoised_uninf)\n",
        "visualize_preprocessing(clean_uninf, noisy_uninf, denoised_uninf, edges_uninf, mask_uninf, is_infected_uninf, prefix=\"uninfected\")\n",
        "\n",
        "# Part 2: Dataset Creation\n",
        "class LeishManiaDataset(Dataset):\n",
        "    def __init__(self, num_samples=1000, img_size=256, is_test=False, noise_std=0.2, kalman_a=0.3, kalman_b=0.3, uninfected_prob=0.2, use_kalman_edges=True):\n",
        "        self.num_samples = num_samples\n",
        "        self.img_size = img_size\n",
        "        self.is_test = is_test\n",
        "        self.noise_std = noise_std\n",
        "        self.kalman_a = kalman_a\n",
        "        self.kalman_b = kalman_b\n",
        "        self.uninfected_prob = uninfected_prob\n",
        "        self.use_kalman_edges = use_kalman_edges\n",
        "        self.samples = [self._generate_sample() for _ in range(num_samples)] if self.is_test else None\n",
        "\n",
        "    def _generate_sample(self):\n",
        "        uninfected = np.random.rand() < self.uninfected_prob\n",
        "        clean, mask, is_infected = generate_synthetic_data(self.img_size, uninfected=uninfected)\n",
        "        noisy = add_gaussian_noise(clean, std=self.noise_std)\n",
        "        if self.use_kalman_edges:\n",
        "            denoised = kalman_filter(noisy, a=self.kalman_a, b=self.kalman_b)\n",
        "            edges = sobel_edges(denoised)\n",
        "            inputs = torch.tensor(np.stack([noisy, denoised, edges], axis=0), dtype=torch.float32)\n",
        "        else:\n",
        "            inputs = torch.tensor(noisy, dtype=torch.float32).unsqueeze(0)\n",
        "        target = torch.tensor(mask, dtype=torch.float32).unsqueeze(0)\n",
        "        return inputs, target, is_infected\n",
        "\n",
        "    def __len__(self): return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.is_test and self.samples is not None:\n",
        "            return self.samples[idx][0], self.samples[idx][1]\n",
        "        else:\n",
        "            inputs, target, _ = self._generate_sample()\n",
        "            return inputs, target\n",
        "\n",
        "NOISE_STD = 0.2\n",
        "KALMAN_A = 0.3\n",
        "KALMAN_B = 0.3\n",
        "UNINFECTED_PROB = 0.2\n",
        "\n",
        "train_dataset_knet = LeishManiaDataset(num_samples=800, noise_std=NOISE_STD, kalman_a=KALMAN_A, kalman_b=KALMAN_B, uninfected_prob=UNINFECTED_PROB, use_kalman_edges=True)\n",
        "val_dataset_knet = LeishManiaDataset(num_samples=200, is_test=True, noise_std=NOISE_STD, kalman_a=KALMAN_A, kalman_b=KALMAN_B, uninfected_prob=UNINFECTED_PROB, use_kalman_edges=True)\n",
        "test_dataset_knet = LeishManiaDataset(num_samples=200, is_test=True, noise_std=NOISE_STD, kalman_a=KALMAN_A, kalman_b=KALMAN_B, uninfected_prob=UNINFECTED_PROB, use_kalman_edges=True)\n",
        "\n",
        "train_dataset_base = LeishManiaDataset(num_samples=800, noise_std=NOISE_STD, kalman_a=KALMAN_A, kalman_b=KALMAN_B, uninfected_prob=UNINFECTED_PROB, use_kalman_edges=False)\n",
        "val_dataset_base = LeishManiaDataset(num_samples=200, is_test=True, noise_std=NOISE_STD, kalman_a=KALMAN_A, kalman_b=KALMAN_B, uninfected_prob=UNINFECTED_PROB, use_kalman_edges=False)\n",
        "test_dataset_base = LeishManiaDataset(num_samples=200, is_test=True, noise_std=NOISE_STD, kalman_a=KALMAN_A, kalman_b=KALMAN_B, uninfected_prob=UNINFECTED_PROB, use_kalman_edges=False)\n",
        "\n",
        "train_loader_knet = DataLoader(train_dataset_knet, batch_size=8, shuffle=True)\n",
        "val_loader_knet = DataLoader(val_dataset_knet, batch_size=8, shuffle=False)\n",
        "test_loader_knet = DataLoader(test_dataset_knet, batch_size=8, shuffle=False)\n",
        "\n",
        "train_loader_base = DataLoader(train_dataset_base, batch_size=8, shuffle=True)\n",
        "val_loader_base = DataLoader(val_dataset_base, batch_size=8, shuffle=False)\n",
        "test_loader_base = DataLoader(test_dataset_base, batch_size=8, shuffle=False)\n",
        "\n",
        "# Part 3: Model Definitions\n",
        "class KNetSeg(nn.Module):\n",
        "    def __init__(self, in_channels=3):\n",
        "        super().__init__()\n",
        "        def conv_block(in_c, out_c):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_c, out_c, 3, padding=1),\n",
        "                nn.BatchNorm2d(out_c),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(out_c, out_c, 3, padding=1),\n",
        "                nn.BatchNorm2d(out_c),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "        self.enc1 = conv_block(in_channels, 64)\n",
        "        self.enc2 = conv_block(64, 128)\n",
        "        self.enc3 = conv_block(128, 256)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.bottleneck = conv_block(256, 512)\n",
        "        self.up3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
        "        self.dec3 = conv_block(512, 256)\n",
        "        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
        "        self.dec2 = conv_block(256, 128)\n",
        "        self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
        "        self.dec1 = conv_block(128, 64)\n",
        "        self.final = nn.Conv2d(64, 1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(self.pool(e1))\n",
        "        e3 = self.enc3(self.pool(e2))\n",
        "        b = self.bottleneck(self.pool(e3))\n",
        "        d3 = self.dec3(torch.cat([self.up3(b), e3], dim=1))\n",
        "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
        "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
        "        return self.final(d1)\n",
        "\n",
        "class BaselineUNet(nn.Module):\n",
        "    def __init__(self, in_channels=1):\n",
        "        super().__init__()\n",
        "        def conv_block(in_c, out_c):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_c, out_c, 3, padding=1),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(out_c, out_c, 3, padding=1),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "        self.enc1 = conv_block(in_channels, 64)\n",
        "        self.enc2 = conv_block(64, 128)\n",
        "        self.enc3 = conv_block(128, 256)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.bottleneck = conv_block(256, 512)\n",
        "        self.up3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
        "        self.dec3 = conv_block(512, 256)\n",
        "        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
        "        self.dec2 = conv_block(256, 128)\n",
        "        self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
        "        self.dec1 = conv_block(128, 64)\n",
        "        self.final = nn.Conv2d(64, 1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(self.pool(e1))\n",
        "        e3 = self.enc3(self.pool(e2))\n",
        "        b = self.bottleneck(self.pool(e3))\n",
        "        d3 = self.dec3(torch.cat([self.up3(b), e3], dim=1))\n",
        "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
        "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
        "        return self.final(d1)\n",
        "\n",
        "# Loss Function\n",
        "class HybridLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.7):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "    def forward(self, pred, target):\n",
        "        bce_loss = nn.functional.binary_cross_entropy_with_logits(pred, target)\n",
        "        pred_sigmoid = torch.sigmoid(pred)\n",
        "        intersection = (pred_sigmoid * target).sum()\n",
        "        dice_loss = 1 - (2. * intersection + 1e-5) / (pred_sigmoid.sum() + target.sum() + 1e-5)\n",
        "        total_loss = self.alpha * dice_loss + (1 - self.alpha) * bce_loss\n",
        "        return total_loss, bce_loss, dice_loss\n",
        "\n",
        "# Early Stopping\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, min_delta=0.001):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = float('inf')\n",
        "        self.early_stop = False\n",
        "        self.best_state = None\n",
        "    def __call__(self, val_loss, model):\n",
        "        if val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.best_state = model.state_dict()\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "# Training Function with Loss Components\n",
        "def train(model, train_loader, val_loader, epochs, device, model_name):\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)  # Removed verbose\n",
        "    early_stopping = EarlyStopping(patience=5, min_delta=0.005)\n",
        "    scaler = GradScaler('cuda')\n",
        "    criterion = HybridLoss(alpha=0.8)\n",
        "\n",
        "    train_losses, train_bce_losses, train_dice_losses = [], [], []\n",
        "    val_losses, val_bce_losses, val_dice_losses, val_dices = [], [], [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        epoch_bce_loss = 0.0\n",
        "        epoch_dice_loss = 0.0\n",
        "        train_preds, train_targets = [], []\n",
        "\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            with autocast('cuda'):\n",
        "                outputs = model(inputs)\n",
        "                total_loss, bce_loss, dice_loss = criterion(outputs, targets)\n",
        "            scaler.scale(total_loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            epoch_loss += total_loss.item()\n",
        "            epoch_bce_loss += bce_loss.item()\n",
        "            epoch_dice_loss += dice_loss.item()\n",
        "\n",
        "            preds = torch.sigmoid(outputs) > 0.5\n",
        "            train_preds.extend(preds.cpu().numpy().flatten())\n",
        "            train_targets.extend(targets.cpu().numpy().flatten())\n",
        "\n",
        "        val_loss, val_bce, val_dice, val_dice_score = validate(model, val_loader, device, criterion)\n",
        "        train_losses.append(epoch_loss / len(train_loader))\n",
        "        train_bce_losses.append(epoch_bce_loss / len(train_loader))\n",
        "        train_dice_losses.append(epoch_dice_loss / len(train_loader))\n",
        "        val_losses.append(val_loss)\n",
        "        val_bce_losses.append(val_bce)\n",
        "        val_dice_losses.append(val_dice)\n",
        "        val_dices.append(val_dice_score)\n",
        "\n",
        "        print(f\"{model_name} Epoch {epoch+1}/{epochs} | \"\n",
        "              f\"Train Loss: {train_losses[-1]:.4f} (BCE: {train_bce_losses[-1]:.4f}, Dice: {train_dice_losses[-1]:.4f}) | \"\n",
        "              f\"Val Loss: {val_loss:.4f} (BCE: {val_bce:.4f}, Dice: {val_dice:.4f}) | Val Dice Score: {val_dice_score:.4f}\")\n",
        "\n",
        "        scheduler.step(val_dice_score)\n",
        "        early_stopping(val_loss, model)\n",
        "        if early_stopping.early_stop:\n",
        "            print(f\"{model_name} Early stopping triggered\")\n",
        "            model.load_state_dict(early_stopping.best_state)\n",
        "            break\n",
        "        torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, f'{model_name}_best_model.pth'))\n",
        "\n",
        "    torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, f'{model_name}_final.pth'))\n",
        "    return model, train_losses, train_bce_losses, train_dice_losses, val_losses, val_bce_losses, val_dice_losses, val_dices\n",
        "\n",
        "# Validation Function with Loss Components\n",
        "def validate(model, loader, device, criterion):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_bce = 0.0\n",
        "    val_dice = 0.0\n",
        "    total_dice_score = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            with autocast('cuda'):\n",
        "                outputs = model(inputs)\n",
        "                total_loss, bce_loss, dice_loss = criterion(outputs, targets)\n",
        "            preds = torch.sigmoid(outputs)\n",
        "            intersection = (preds * targets).sum()\n",
        "            dice_score = (2. * intersection) / (preds.sum() + targets.sum() + 1e-5)\n",
        "            val_loss += total_loss.item()\n",
        "            val_bce += bce_loss.item()\n",
        "            val_dice += dice_loss.item()\n",
        "            total_dice_score += dice_score.item()\n",
        "    return (val_loss / len(loader), val_bce / len(loader), val_dice / len(loader), total_dice_score / len(loader))\n",
        "\n",
        "# Testing Function with Metrics and Loss Components\n",
        "def test(model, test_loader, device, criterion, model_name):\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    test_bce = 0.0\n",
        "    test_dice = 0.0\n",
        "    test_dice_score = 0.0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            total_loss, bce_loss, dice_loss = criterion(outputs, targets)\n",
        "            preds = torch.sigmoid(outputs) > 0.5\n",
        "\n",
        "            test_loss += total_loss.item()\n",
        "            test_bce += bce_loss.item()\n",
        "            test_dice += dice_loss.item()\n",
        "            intersection = (preds * targets).sum()\n",
        "            dice_score = (2. * intersection) / (preds.sum() + targets.sum() + 1e-5)\n",
        "            test_dice_score += dice_score.item()\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy().flatten())\n",
        "            all_targets.extend(targets.cpu().numpy().flatten())\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    test_bce /= len(test_loader)\n",
        "    test_dice /= len(test_loader)\n",
        "    test_dice_score /= len(test_loader)\n",
        "    precision = precision_score(all_targets, all_preds, zero_division=0)\n",
        "    recall = recall_score(all_targets, all_preds, zero_division=0)\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    cm = confusion_matrix(all_targets, all_preds)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    iou = tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 0\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "    print(f\"{model_name} Test Loss: {test_loss:.4f} (BCE: {test_bce:.4f}, Dice: {test_dice:.4f}) | \"\n",
        "          f\"Test Dice Score: {test_dice_score:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f} | \"\n",
        "          f\"F1: {f1:.4f} | IoU: {iou:.4f} | Specificity: {specificity:.4f}\")\n",
        "    print(f\"{model_name} Confusion Matrix: TN={tn}, FP={fp}, FN={fn}, TP={tp}\")\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f'{model_name} Test Set Confusion Matrix')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, f'{model_name}_test_confusion_matrix.png'))\n",
        "    plt.close()\n",
        "\n",
        "    return test_loss, test_bce, test_dice, test_dice_score, precision, recall, f1, iou, specificity, cm\n",
        "\n",
        "# Visualization Function\n",
        "def visualize_test_results(model, test_loader, device, model_name, num_examples=3):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, targets) in enumerate(test_loader):\n",
        "            if i >= num_examples:\n",
        "                break\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            preds = torch.sigmoid(outputs) > 0.5\n",
        "\n",
        "            fig, axes = plt.subplots(1, 4 if model_name == \"KNetSeg\" else 2, figsize=(20 if model_name == \"KNetSeg\" else 10, 5))\n",
        "            if model_name == \"KNetSeg\":\n",
        "                axes[0].imshow(inputs[0, 0].cpu().numpy(), cmap='gray')\n",
        "                axes[0].set_title('Noisy Input')\n",
        "                axes[1].imshow(inputs[0, 1].cpu().numpy(), cmap='gray')\n",
        "                axes[1].set_title('Denoised Input')\n",
        "                axes[2].imshow(inputs[0, 2].cpu().numpy(), cmap='gray')\n",
        "                axes[2].set_title('Edge Input')\n",
        "                axes[3].imshow(preds[0, 0].cpu().numpy(), cmap='gray')\n",
        "                axes[3].set_title('Predicted Mask')\n",
        "            else:\n",
        "                axes[0].imshow(inputs[0, 0].cpu().numpy(), cmap='gray')\n",
        "                axes[0].set_title('Noisy Input')\n",
        "                axes[1].imshow(preds[0, 0].cpu().numpy(), cmap='gray')\n",
        "                axes[1].set_title('Predicted Mask')\n",
        "            for ax in axes:\n",
        "                ax.axis('off')\n",
        "            plt.savefig(os.path.join(OUTPUT_DIR, f'{model_name}_test_example_{i+1}.png'))\n",
        "            plt.close()\n",
        "\n",
        "# Train and Test Both Models\n",
        "EPOCHS = 50\n",
        "\n",
        "# KNetSeg\n",
        "set_seed(42)\n",
        "knet_model = KNetSeg(in_channels=3).to(DEVICE)\n",
        "knet_trained, knet_train_losses, knet_train_bce_losses, knet_train_dice_losses, knet_val_losses, knet_val_bce_losses, knet_val_dice_losses, knet_val_dices = train(\n",
        "    knet_model, train_loader_knet, val_loader_knet, EPOCHS, DEVICE, \"KNetSeg\"\n",
        ")\n",
        "knet_model.load_state_dict(torch.load(os.path.join(OUTPUT_DIR, 'KNetSeg_best_model.pth'), weights_only=True))\n",
        "knet_test_results = test(knet_model, test_loader_knet, DEVICE, HybridLoss(), \"KNetSeg\")\n",
        "visualize_test_results(knet_model, test_loader_knet, DEVICE, \"KNetSeg\")\n",
        "\n",
        "# BaselineUNet\n",
        "set_seed(42)\n",
        "base_model = BaselineUNet(in_channels=1).to(DEVICE)\n",
        "base_trained, base_train_losses, base_train_bce_losses, base_train_dice_losses, base_val_losses, base_val_bce_losses, base_val_dice_losses, base_val_dices = train(\n",
        "    base_model, train_loader_base, val_loader_base, EPOCHS, DEVICE, \"BaselineUNet\"\n",
        ")\n",
        "base_model.load_state_dict(torch.load(os.path.join(OUTPUT_DIR, 'BaselineUNet_best_model.pth'), weights_only=True))\n",
        "base_test_results = test(base_model, test_loader_base, DEVICE, HybridLoss(), \"BaselineUNet\")\n",
        "visualize_test_results(base_model, test_loader_base, DEVICE, \"BaselineUNet\")\n",
        "\n",
        "# Comparison Plots\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(1, len(knet_train_losses) + 1), knet_train_losses, label='KNetSeg Train Loss')\n",
        "plt.plot(range(1, len(base_train_losses) + 1), base_train_losses, label='BaselineUNet Train Loss')\n",
        "plt.plot(range(1, len(knet_val_losses) + 1), knet_val_losses, label='KNetSeg Val Loss')\n",
        "plt.plot(range(1, len(base_val_losses) + 1), base_val_losses, label='BaselineUNet Val Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, 'loss_comparison.png'))\n",
        "plt.close()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(1, len(knet_val_dices) + 1), knet_val_dices, label='KNetSeg Val Dice')\n",
        "plt.plot(range(1, len(base_val_dices) + 1), base_val_dices, label='BaselineUNet Val Dice')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Dice Score')\n",
        "plt.title('Validation Dice Score Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, 'dice_comparison.png'))\n",
        "plt.close()\n",
        "\n",
        "# Plot Loss Components\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(1, len(knet_train_bce_losses) + 1), knet_train_bce_losses, label='KNetSeg Train BCE')\n",
        "plt.plot(range(1, len(knet_train_dice_losses) + 1), knet_train_dice_losses, label='KNetSeg Train Dice')\n",
        "plt.plot(range(1, len(base_train_bce_losses) + 1), base_train_bce_losses, label='BaselineUNet Train BCE')\n",
        "plt.plot(range(1, len(base_train_dice_losses) + 1), base_train_dice_losses, label='BaselineUNet Train Dice')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training BCE and Dice Loss Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, 'train_loss_components.png'))\n",
        "plt.close()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(1, len(knet_val_bce_losses) + 1), knet_val_bce_losses, label='KNetSeg Val BCE')\n",
        "plt.plot(range(1, len(knet_val_dice_losses) + 1), knet_val_dice_losses, label='KNetSeg Val Dice')\n",
        "plt.plot(range(1, len(base_val_bce_losses) + 1), base_val_bce_losses, label='BaselineUNet Val BCE')\n",
        "plt.plot(range(1, len(base_val_dice_losses) + 1), base_val_dice_losses, label='BaselineUNet Val Dice')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Validation BCE and Dice Loss Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, 'val_loss_components.png'))\n",
        "plt.close()\n",
        "\n",
        "print(f\"Training and testing complete for both models. Results and comparisons saved in {OUTPUT_DIR}\")"
      ]
    }
  ]
}